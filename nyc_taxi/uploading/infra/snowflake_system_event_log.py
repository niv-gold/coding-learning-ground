from __future__ import annotations
from snowflake import connector
from nyc_taxi.uploading.core.models import FileIdentity
from nyc_taxi.uploading.core.ports import LoadLogRepository
from nyc_taxi.uploading.config.settings import SnowflakeConfig
from nyc_taxi.uploading.infra.local_finder import LocalFileFinder
import uuid
from pathlib import Path
from datetime import datetime
import json 
from dotenv import load_dotenv

class SnowflakeLoadLogRepository(LoadLogRepository):
    """
    Uses SYSTEM_EVENT_LOG table to:
    - detect already loaded files (entity_type='FILE', event_type='INGEST', status='SUCCESS')
    - write success/failure events for each file
    """

    def __init__(self, conn_params: dict):
        self.conn_params = conn_params
        self.log_table = conn_params["log_table"]
        self.log_schema = conn_params["log_schema"]

    def _connect(self):
        return connector.connect(**self.conn_params)

    def already_loaded(self, file_keys: list[str])-> set[str]:        
        """
            entity_id - holds the stable_key() output: {self.name}|{self.size_byte}|{int(self.modified_time.timestamp())}
                        (Exaple: taxi_zone_lookup.csv|12345|1767285799)
            entity_id - (Example: taxi_zone_lookup.csv)
        """
        
        if not file_keys:
            return set()

        # Use VALUES binding pattern to avoid unsafe string concat
        sql_place_holders = ", ".join(['(%s)'] * len(file_keys))
        sql = f"""
            SELECT distinct entity_id
            FROM {self.log_schema}.{self.log_table}
            WHERE entity_id IN (SELECT column1 FROM VALUES {sql_place_holders})
                AND entity_type = 'FILE'
                AND event_type  = 'INGEST'
                AND status = 'SUCCESS'
        """

        with self._connect() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, file_keys)
                rows = cur.fetchall()
        return { file[0] for file in rows }        

    def _insert_event(self, *, event_id: str, run_id: str, event_level: str, event_type: str, component: str, entity_type: str, 
                      entity_id: str, status: str, message: str, error_code: str | None, error_details: str | None, metadata: dict)-> None:
        """
        Insert a row into SYSTEM_EVENT_LOG.
        """
        # We store metadata as VARIANT; easiest is to pass JSON string and PARSE_JSON in SQL.
        metadata_json = json.dumps(metadata or {}, allow_nan=False, separators=(",", ":"), ensure_ascii=False)

        sql = f"""
            INSERT INTO {self.log_schema}.{self.log_table} (
                event_id,
                run_id,
                event_timestamp,
                event_level,
                event_type,
                component,
                entity_type,
                entity_id,
                status,
                message,
                error_code,
                error_details,
                metadata
            ) 
            VALUES(
            %s, %s, CURRENT_TIMESTAMP(),
                %s, %s, %s,
                %s, %s,
                %s, %s,
                %s, %s,
                PARSE_JSON(%s)
            )
            """
        
        params = (
            event_id, run_id,
            event_level, event_type, component,
            entity_type, entity_id,
            status, message,
            error_code, error_details,
            metadata_json
        )

        with self._connect() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, params)

    def log_success(self, run_id: str, entity_type: str, entity_id: str, component: str, message: str, metadata: dict)-> None:
        # event_id should be generated by caller (pipeline) so it can be correlated if needed
        # but to keep the interface simple, we can generate it here if you prefer.
        raise NotImplementedError("Use log_event_success(...) below or modify to generate event_id")
    
    def log_failure(self, run_id: str, entity_type: str, entity_id: str, component: str, message: str, error_details: str, metadata: dict)-> None:
        raise NotImplementedError("Use log_event_failure(...) below or modify to generate event_id")

    # Practical methods â€” explicitly include event_id + standard fields
    def log_ingest_success(self, *, event_id: str, run_id: str, component: str, entity_id: str, message: str, metadata: dict)-> None:
        self._insert_event(
                event_id= 
                run_id= ,
                event_timestamp= ,
                event_level= ,
                event_type= ,
                component= ,
                entity_type= ,
                entity_id= ,
                status= ,
                message= ,
                error_code= ,
                error_details= ,
                metadata= 
        )

    def log_ingest_failure(elf, run_id: str, entity_type: str, entity_id: str, component: str, message: str, error_details: str, metadata: dict)-> None:
        pass


if __name__=='__main__':
    load_dotenv()

    path = Path('/home/niv/home/GitHubeRepos/my_codes/nyc_taxi/uploading/app/data_files')
    files = LocalFileFinder(path).list_files()
    list_files_stable_key = [file.stable_key for file in files]

    snowflake_config = SnowflakeConfig.from_env().to_connector_kwarg()
    sn_inst = SnowflakeLoadLogRepository(conn_params=snowflake_config)

    sn_inst._insert_event(event_id='1', run_id='a', event_level='')




    # already_loded = sn_inst.already_loaded(list_files_stable_key)
    # print(already_loded)

    # sql_exec_1 ="""
    # INSERT INTO NYC_TAXI.SYSTEM_EVENT_LOG (event_id, run_id, event_timestamp, event_level, event_type, component, entity_type, entity_id, status, message, error_code, error_details, metadata)
    # SELECT 
    #     'evt_123',
    #     'run_456',
    #     CURRENT_TIMESTAMP(),
    #     'INFO',
    #     'PROCESS_START',
    #     'ETL_PIPELINE',
    #     'JOB',
    #     'taxi_zone_lookup.csv|12331|1766611598',
    #     'SUCCESS',
    #     'Process started successfully',
    #     NULL,
    #     NULL,
    #     PARSE_JSON('{"key":"value"}')
    # """
